# -*- coding: utf-8 -*-
"""barcode_computervision_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Ci61-hooqwO0f8UD80tAkT0iOt8lNom
"""

#unpacking all the files into seperate folders

import os
from zipfile import ZipFile
file_name = "/content/N95-2592x1944_scaledTo800x600NN_v1.zip"
with ZipFile(file_name, 'r') as zip:
  zip.printdir()
  zip.extractall(path  = 'TestImages')

file_name2 = "/content/barcode_bb.zip"
with ZipFile(file_name2, 'r') as zip:
  zip.printdir()
  zip.extractall(path = 'TrainImages')

#import os
from PIL import Image
from pyzbar.pyzbar import decode
#1055 images in test set
filepath = 'Images/N95-2592x1944_scaledTo800x600NN'
x = 0
score = 0
#go through each image, for each:
#open the image, get the real value of the barcode from the image title.
#use the pyzbar.decode function to find and decode the images
#it returns a list of each barcode in the image, all the images in the dataset only have one barcode
#if the lis tis null, then it couldn't recognize a barcode in the image, we treat that as a miss
#increment score if it matches
for filename in os.listdir(filepath):
  f = os.path.join(filepath, filename)
  x = x+1
  if os.path.isfile(f):
    #print(f)
    #get barcode number
    img = Image.open(f)
    #print(img)
    realcode = f[39:52]
    #print(realcode)
    decoded_list = decode(img)
    if not (decoded_list):
      code = '0'
      print(code)

    else:
      code = (decoded_list[0].data.decode())
      #print(code)
    if (code == realcode):
      score = score+1
print(score)
print(x)
print(score/x)

#77 accuracy
# the ones it missed where due to pyzbar not being able to recognize the barcode
#seems when it could recognize a barcode, it decoded it correctly.

#might have to do this to get pyzbar to work, it is finicky
!apt install libzbar0
!pip install pyzbar

import pickle
from PIL import Image
import os
import numpy as np
trainImages = []
trainLabels = []
filepath = 'TrainImages/barcode_bb/single_test'
for filename in os.listdir(filepath):
  f = os.path.join(filepath, filename)
  if os.path.isfile(f):
    img = Image.open(f).convert('L')

    #make everything same size for neural net. May distort figure, problem with datasets, hard to find images with same sizes.
    resized = img.resize((200, 150))
    #print(resized.mode)
    resizedarr = np.array(resized)
    #print(resizedarr.shape)
    #get labelvalue from filename
    label = f[35:48]

    labellist = list(map(int, str(label)))
    trainLabels.append(labellist)
    trainImages.append(resizedarr)
    #print(labellist)

#pickle the images and their labels for ease of use.
with open('trainimages.pkl', 'wb') as tif:  # open a text file
    pickle.dump(trainImages, tif)
with open('trainlabels.pkl', 'wb') as tlf:
    pickle.dump(trainLabels, tlf)

import os
import pickle
from PIL import Image
import numpy as np
testImages = []
testLabels = []
#does the same as previous block but for the test images.
filepath2 = '/content/TestImages/N95-2592x1944_scaledTo800x600NN'
for filename in os.listdir(filepath2):

  f = os.path.join(filepath2, filename)
  if os.path.isfile(f):

    img = Image.open(f).convert('L')
    resized = img.resize((200, 150))
    resizedarr = np.array(resized)

    label = (f[52:65])
    labellist = list(map(int, str(label)))
    testLabels.append(labellist)
    testImages.append(resizedarr)


with open('testimages.pkl', 'wb') as tif2:
  pickle.dump(testImages, tif2)
with open('testlabels.pkl', 'wb') as tlf2:
  pickle.dump(testLabels, tlf2)

import sys
import pickle
import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import itertools
import tensorflow as tf
import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, Reshape, Softmax
from keras.optimizers import RMSprop,Adam
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.utils import to_categorical
from sklearn.neighbors import KNeighborsClassifier

from keras.layers import LeakyReLU
import random
from PIL import Image

#rezise and list imags
with open('trainimages.pkl', 'rb') as f:

    TrainImages = pickle.load(f)
with open('trainlabels.pkl', 'rb') as f2:
    TrainLabels = pickle.load(f2)

with open('testimages.pkl','rb') as f3:
    TestImages = pickle.load(f3)

with open ('testlabels.pkl', 'rb') as f4:
    TestLabels = pickle.load(f4)


#print((TrainImages))
#convert to np.array
TrainImagesArr = np.array(TrainImages)
TrainLabelsArr = np.array(TrainLabels)
TestImagesArr = np.array(TestImages)
TestLabelsArr = np.array(TestLabels)

#print(TrainImagesArr)
print(TrainImagesArr.shape)
TrainImagesArr = np.stack(TrainImagesArr)
print(TrainImagesArr.shape)
#reshape into right size for cnn
TrainImagess = TrainImagesArr.reshape(-1, 150, 200, 1)
TestImagesss = TestImagesArr.reshape(-1, 150, 200, 1)


#TrainImages = TrainImages.astype('float32')
#TrainImages = TrainImages/255
#convert labels into onehot encoded matrices
print(TestLabelsArr.shape)
labelonehot = to_categorical(TrainLabelsArr)

testlabelonehot = to_categorical(TestLabelsArr)
print(labelonehot[0])

#split between train and validation
train, validation, trainlabel, validationlabel = train_test_split(TrainImagess, labelonehot, test_size = 0.2, random_state =10  )
print(type(train))
print(train.shape)
print(trainlabel.shape)
model = Sequential()
#3layers of convolution and maxpooling layers, increasing in size
model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',
                 activation ='relu', input_shape = (150,200,1)))
model.add(MaxPool2D(pool_size=(2,2), strides = (1,1)))
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides = (1,1)))
model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'Same', activation = 'relu'))
model.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))
#flatten output
model.add(Flatten())
#ideally this would be like 4096 neurons, model cant handle it
model.add(Dense(200, activation = 'relu'))
model.add(Dense(130, activation = 'relu'))
#reshape into correct format
model.add(Reshape((13,10)))
#softmax gives probabilities
model.add(Softmax())

model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'] )

result = model.fit(train, trainlabel, batch_size =65, epochs = 20, validation_data = (validation, validationlabel) )
predict = model.predict(TestImagesss)
result = (np.round(predict))
#print(predict[7])
#for r in result:
  #max_index = np.argmax(r)

 # r.flat[max_index] = 1
 #r[r != r.flat[max_index]] = 0
#for x in range(1055):
   # print(result[x])
  #  print(testlabelonehot[x])

()